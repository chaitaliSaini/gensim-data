# Gensim data
This repository contains models and dataset that available through [gensim](https://github.com/RaRe-Technologies/gensim) download api. This repo used as storage.

## Quickstart
To load model/dataset you can use 2 types of API:
- CLI
  ```bash
  python -m gensim.downloader --catalogue  # show info about available models/datasets
  python -m gensim.downloader --download glove-twitter-25  # download model, will be stored in ~/gensim-data/glove-twitter-50/
  ```
- Python API

  Example: load pre-trained vectors
  ```python
  import gensim.downloader as api

  info = api.info()  # show info about available models/datasets
  model = api.load("glove-twitter-25")  # download model and load it to memory
  model.most_similar("cat")
  
  """
  Output:

  [(u'dog', 0.9590819478034973),
   (u'monkey', 0.9203578233718872),
   (u'bear', 0.9143137335777283),
   (u'pet', 0.9108031392097473),
   (u'girl', 0.8880630135536194),
   (u'horse', 0.8872727155685425),
   (u'kitty', 0.8870542049407959),
   (u'puppy', 0.886769711971283),
   (u'hot', 0.8865255117416382),
   (u'lady', 0.8845518827438354)]
 
  """
  ```
  
  Example: load dataset and train Word2Vec
  ```python
  from gensim.models.word2vec import Text8Corpus
  from gensim.models.word2vec import Word2Vec
  import gensim.downloader as api

  corpus_path = api.load('text8')  # download dataset
  corpus = Text8Corpus(corpus_path)  # load text8 as corpus
  model = Word2Vec(corpus)  # train model

  ```
  
## Available data
### Datasets
| name | source | description |
|------|--------|-------------|
| 20-newsgroups | http://qwone.com/~jason/20Newsgroups/ | The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups |
| fake-news | Kaggle | It contains text and metadata scraped from 244 websites tagged as 'bullshit' here by the BS Detector Chrome Extension by Daniel Sieradski. |
| text8 | http://mattmahoney.net/dc/text8.zip | Cleaned small sample from wikipedia |

### Models
| name | description | papers | preprocessing | parameters |
|------|-------------|------------|--------|---------------|
| glove-twitter-100 | Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-100.txt` | dimensions = 100 |
| glove-twitter-200 | Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-200.txt` | dimensions = 200 |
| glove-twitter-25 | Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt` | dimensions = 25 |
| glove-twitter-50 | Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-50.txt` | dimensions = 50 |
| glove-wiki-gigaword-100 | Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-100.txt` | dimensions = 100 |
| glove-wiki-gigaword-200 | Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-200.txt` | dimentions = 200 |
| glove-wiki-gigaword-300 | Pre-trained vectors, Wikipedia 2014 + Gigaword 5, 6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-300.txt` | dimensions = 300 |
| glove-wiki-gigaword-50 | Pre-trained vectors ,Wikipedia 2014 + Gigaword 5,6B tokens, 400K vocab, uncased. https://nlp.stanford.edu/projects/glove/ | https://nlp.stanford.edu/pubs/glove.pdf | Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-wiki-gigaword-50.txt` | dimension = 50 |
(generated by generate_table.py based on lists.json)
  
## How to add new dataset/model ?
1. Create the `tar.gz` file of the dataset/model. 
   ```bash
   tar -cvzf - filename > filename.tar.gz
   ```
   Attention, if your dataset/model size is greater than **1800MB**, use `split` after:
   ```bash
   tar -cvzf - filename | split -d -b 1800m - filename.tar.gz_
   ```
2. Share your archives with any file-sharing service.
   
3. Create the ```__init__.py``` file for dataset/model, ```__init__.py``` **must** contains `load_data()` function. 

   For **datasets**, this function **returns the path to dataset file/folder**, 
for **model**, `load_data()` **returns loaded to memory model**. 
   
   Add this file to your branch, create PR, add links from (2), ping **[@menhikh-iv](https://github.com/menshikh-iv)**.

4. For **[@menhikh-iv](https://github.com/menshikh-iv)**: calculate checksum of your archive (or parts), you can use `md5sum filename.tar.gz` or this python function: 
   ```python
   import hashlib
   
   def calculate_md5_checksum(tar_file):
       hash_md5 = hashlib.md5()
       with open(tar_file, "rb") as f:
           for chunk in iter(lambda: f.read(4096), b""):
               hash_md5.update(chunk)
       return hash_md5.hexdigest()
    
   print(calculate_md5_checksum("filename.tar.gz"))
   ```
   
5. For **[@menhikh-iv](https://github.com/menshikh-iv)**: check that models loads correctly
6. For **[@menhikh-iv](https://github.com/menshikh-iv)**: create release, attach archives + `__init__.py`
7. For **[@menhikh-iv](https://github.com/menshikh-iv)**: update `lists.json` (load json with python, add and save), fill-up all fields:
   - Example (dataset)
   ```
   u'text8': {
        u'source': u'http://mattmahoney.net/dc/text8.zip',  # link to original resourse with dataset
        u'checksum': u'f407f5aed497fc3b0fb33b98c4f9d855',  # hash (from step 2)
        u'parts': 1,  # number of archieve part
        u'description': u'Cleaned small sample from wikipedia',  # description of dataset
        u'file_name': u'text8'  # filename in archive
   }
   ```
   - Example (model)
   ```
   'glove-twitter-25': {
	    u'description': u'Pre-trained vectors, 2B tweets, 27B tokens, 1.2M vocab, uncased. https://nlp.stanford.edu/projects/glove/', 
	    u'parameters': u'dimensions = 25',  # info about parameters (if it's known)
	    u'file_name': u'glove-twitter-25.txt', 
	    u'papers': u'https://nlp.stanford.edu/pubs/glove.pdf',  # link to papers
	    u'parts': 1, 
	    u'preprocessing': u'Converted to w2v format with `python -m gensim.scripts.glove2word2vec -i <fname> -o glove-twitter-25.txt`',  # info about preprocessing (if it's known)
	    u'checksum': u'5c3784f59a8a7761059342dc32f8f9e6'
   }
   ```
